"""
Advanced Data Preprocessing Module for BudgetWise Forecasting System
Implements comprehensive data loading, cleaning, merging, and validation based on notebook analysis.
Handles multiple datasets with intelligent duplicate resolution, data quality improvement, and standardization.
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import yaml
from datetime import datetime, timedelta
import warnings
import re
from collections import Counter
from scipy import stats

# Try to import rapidfuzz for fuzzy matching, fallback to basic matching if not available
try:
    from rapidfuzz import process
    FUZZY_MATCHING_AVAILABLE = True
except ImportError:
    FUZZY_MATCHING_AVAILABLE = False

warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if not FUZZY_MATCHING_AVAILABLE:
    logger.warning("rapidfuzz not available - using basic string matching for category standardization")

class AdvancedDataPreprocessor:
    """
    Advanced data preprocessing for BudgetWise financial data with multi-dataset support.
    Implements comprehensive cleaning, merging, and quality improvement strategies.
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """Initialize the data preprocessor with configuration."""
        self.config = self._load_config(config_path)
        self.raw_data_path = Path(self.config['data']['raw_data_path'])
        self.processed_data_path = Path(self.config['data']['processed_data_path'])
        self.processed_data_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize data containers
        self.datasets = {}
        self.combined_data = None
        self.processed_data = None
        
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration from YAML file."""
        try:
            with open(config_path, 'r') as file:
                return yaml.safe_load(file)
        except FileNotFoundError:
            logger.warning(f"Config file {config_path} not found. Using default settings.")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """Return default configuration."""
        return {
            'data': {
                'raw_data_path': 'data/raw/',
                'processed_data_path': 'data/processed/',
                'train_split': 0.7,
                'val_split': 0.15,
                'test_split': 0.15
            },
            'preprocessing': {
                'duplicate_threshold': 0.95,
                'outlier_method': 'iqr',
                'outlier_factor': 1.5,
                'date_formats': ['%Y-%m-%d', '%d-%m-%Y', '%m/%d/%Y', '%d/%m/%Y'],
                'amount_patterns': [r'‚Çπ\s*([0-9,]+\.?[0-9]*)', r'\$\s*([0-9,]+\.?[0-9]*)', r'([0-9,]+\.?[0-9]*)']
            }
        }

    def load_multiple_datasets(self) -> Dict[str, pd.DataFrame]:
        """
        Load multiple BudgetWise datasets from raw data directory.
        
        Returns:
            Dictionary containing loaded datasets
        """
        logger.info("üîÑ Loading Multiple BudgetWise Finance Datasets...")
        logger.info("=" * 60)
        
        dataset_files = {
            'primary': 'budgetwise_finance_dataset.csv',
            'secondary': 'budgetwise_synthetic_dirty.csv',
            'combined_original': 'budgetwise_original_combined.csv'
        }
        
        datasets = {}
        
        for dataset_name, filename in dataset_files.items():
            file_path = self.raw_data_path / filename
            
            if file_path.exists():
                try:
                    df = pd.read_csv(file_path)
                    df['dataset_source'] = dataset_name
                    datasets[dataset_name] = df
                    logger.info(f"‚úÖ {dataset_name.title()} dataset loaded: {df.shape[0]:,} records, {df.shape[1]} columns")
                except Exception as e:
                    logger.error(f"‚ùå Error loading {dataset_name} dataset: {e}")
            else:
                logger.warning(f"‚ö†Ô∏è  {dataset_name.title()} dataset not found: {file_path}")
        
        if not datasets:
            raise FileNotFoundError("No datasets found in the raw data directory")
        
        # Store datasets
        self.datasets = datasets
        
        # Log comparison
        logger.info(f"\nüîç Dataset Comparison:")
        for name, df in datasets.items():
            if name != 'combined_original':  # Skip already processed dataset
                main_columns = [col for col in df.columns if col != 'dataset_source']
                logger.info(f"üìã {name.title()} dataset columns: {main_columns}")
        
        logger.info(f"üìä Total records to process: {sum(df.shape[0] for df in datasets.values()):,}")
        
        return datasets

    def merge_datasets(self) -> pd.DataFrame:
        """
        Intelligently merge multiple datasets with conflict resolution.
        
        Returns:
            Combined and cleaned DataFrame
        """
        logger.info("üîó Merging datasets with intelligent conflict resolution...")
        
        # If we have the combined_original dataset, use it as base
        if 'combined_original' in self.datasets:
            logger.info("‚úÖ Using existing combined dataset as base")
            combined_df = self.datasets['combined_original'].copy()
            
            # Remove the extra preprocessing columns to get clean base
            base_columns = ['transaction_id', 'user_id', 'date', 'transaction_type', 
                          'category', 'amount', 'payment_mode', 'location', 'notes']
            
            # Keep dataset source if available
            if 'dataset_source' in combined_df.columns:
                base_columns.append('dataset_source')
            
            # Filter to base columns that exist
            available_columns = [col for col in base_columns if col in combined_df.columns]
            combined_df = combined_df[available_columns].copy()
            
        else:
            # Merge primary and secondary datasets
            datasets_to_merge = []
            
            if 'primary' in self.datasets:
                datasets_to_merge.append(self.datasets['primary'])
            
            if 'secondary' in self.datasets:
                datasets_to_merge.append(self.datasets['secondary'])
            
            if not datasets_to_merge:
                raise ValueError("No datasets available for merging")
            
            # Combine datasets
            combined_df = pd.concat(datasets_to_merge, ignore_index=True)
            logger.info(f"‚úÖ Combined datasets: {combined_df.shape[0]:,} total records")
        
        # Store combined data
        self.combined_data = combined_df.copy()
        
        # Display dataset source distribution
        if 'dataset_source' in combined_df.columns:
            source_dist = combined_df['dataset_source'].value_counts()
            logger.info(f"\nüìã Dataset source distribution:")
            for source, count in source_dist.items():
                percentage = (count / len(combined_df)) * 100
                logger.info(f"   ‚Ä¢ {str(source).title()}: {count:,} records ({percentage:.1f}%)")
        
        return combined_df

    def advanced_duplicate_detection(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Advanced duplicate detection and resolution with intelligent handling.
        
        Args:
            df: Input DataFrame
            
        Returns:
            DataFrame with duplicates resolved
        """
        logger.info("üîç Advanced Duplicate Detection & Resolution...")
        logger.info("=" * 50)
        
        initial_count = len(df)
        
        # Step 1: Remove exact duplicate rows
        logger.info("\nüéØ Step 1: Exact Duplicate Row Detection")
        df_before_dedup = df.copy()
        df = df.drop_duplicates()
        exact_duplicates_removed = initial_count - len(df)
        logger.info(f"   ‚Ä¢ Exact duplicate rows removed: {exact_duplicates_removed:,}")
        logger.info(f"   ‚Ä¢ Remaining records: {len(df):,}")
        
        # Step 2: Analyze transaction ID duplicates
        logger.info("\nüéØ Step 2: Transaction ID Duplicate Analysis")
        if 'transaction_id' in df.columns:
            duplicate_txn_ids = df['transaction_id'].duplicated().sum()
            unique_txn_ids = df['transaction_id'].nunique()
            total_txn_ids = len(df)
            
            logger.info(f"   ‚Ä¢ Total transaction records: {total_txn_ids:,}")
            logger.info(f"   ‚Ä¢ Unique transaction IDs: {unique_txn_ids:,}")
            logger.info(f"   ‚Ä¢ Duplicate transaction IDs: {duplicate_txn_ids:,}")
            logger.info(f"   ‚Ä¢ Duplication rate: {(duplicate_txn_ids/total_txn_ids)*100:.2f}%")
            
            if duplicate_txn_ids > 0:
                logger.info(f"   ‚ö†Ô∏è  Found {duplicate_txn_ids:,} duplicate transaction IDs")
                
                # Step 3: Intelligent duplicate resolution
                logger.info(f"\nüéØ Step 3: Intelligent Duplicate Resolution")
                
                # Create unique transaction IDs for duplicates
                duplicated_mask = df['transaction_id'].duplicated(keep='first')
                duplicate_count = duplicated_mask.sum()
                
                # Generate unique suffixes for duplicates
                df.loc[duplicated_mask, 'transaction_id'] = (
                    df.loc[duplicated_mask, 'transaction_id'] + '_DUP_' + 
                    df.loc[duplicated_mask].groupby('transaction_id').cumcount().add(1).astype(str)
                )
                
                logger.info(f"   ‚úÖ Created unique transaction IDs for {duplicate_count:,} duplicate records")
                
                # Add duplicate flag for analysis
                df['was_duplicate'] = duplicated_mask.astype(int)
            else:
                logger.info("   ‚úÖ No duplicate transaction IDs found")
                df['was_duplicate'] = 0
        else:
            logger.warning("   ‚ö†Ô∏è  No transaction_id column found for duplicate analysis")
            df['was_duplicate'] = 0
        
        # Summary
        logger.info(f"\nüìä Duplicate Resolution Summary:")
        logger.info(f"   ‚Ä¢ Initial records: {initial_count:,}")
        logger.info(f"   ‚Ä¢ Exact duplicates removed: {exact_duplicates_removed:,}")
        logger.info(f"   ‚Ä¢ Final records: {len(df):,}")
        logger.info(f"   ‚Ä¢ Data retention rate: {(len(df)/initial_count)*100:.2f}%")
        
        return df

    def advanced_date_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Advanced date processing with comprehensive parsing, quality validation, and intelligent imputation.
        
        Args:
            df: DataFrame with date columns
            
        Returns:
            DataFrame with properly processed dates
        """
        logger.info("üìÖ Advanced Date Processing & Standardization...")
        logger.info("=" * 50)
        
        if 'date' not in df.columns:
            logger.warning("‚ö†Ô∏è  No 'date' column found in DataFrame")
            return df
        
        # Store original date column for analysis
        df['date_original'] = df['date'].copy()
        
        logger.info("\nüéØ Step 1: Date Format Analysis")
        # Analyze existing date formats
        date_samples = df['date'].dropna().astype(str).head(20).tolist()
        logger.info(f"   üìã Sample date formats found:")
        for i, date_sample in enumerate(date_samples[:10], 1):
            logger.info(f"      {i:2d}. {date_sample}")
        
        # Attempt to parse multiple date formats
        logger.info(f"\nüéØ Step 2: Multi-format Date Parsing")
        date_formats = [
            '%Y-%m-%d',     # 2023-01-15
            '%m/%d/%Y',     # 01/15/2023
            '%d/%m/%Y',     # 15/01/2023
            '%Y/%m/%d',     # 2023/01/15
            '%B %d %Y',     # January 15 2023
            '%d %B %Y',     # 15 January 2023
            '%d-%m-%Y',     # 15-01-2023
        ]
        
        # Primary parsing attempt
        df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=False)
        
        # Count unparseable dates
        unparseable_dates = df['date'].isna().sum()
        parseable_dates = len(df) - unparseable_dates
        parsing_success_rate = (parseable_dates / len(df)) * 100
        
        logger.info(f"   ‚Ä¢ Successfully parsed dates: {parseable_dates:,} ({parsing_success_rate:.1f}%)")
        logger.info(f"   ‚Ä¢ Unparseable dates found: {unparseable_dates:,} ({(unparseable_dates/len(df)*100):.1f}%)")
        
        # Advanced date imputation strategy
        if unparseable_dates > 0:
            logger.info(f"\nüéØ Step 3: Intelligent Date Imputation")
            
            # Use statistical imputation
            valid_dates = df['date'].dropna()
            
            if len(valid_dates) > 0:
                # Multiple imputation strategies
                date_median = valid_dates.median()
                date_mode = valid_dates.mode()[0] if not valid_dates.mode().empty else date_median
                date_mean = valid_dates.mean()
                
                logger.info(f"   üìä Date statistics for imputation:")
                logger.info(f"      ‚Ä¢ Median date: {pd.to_datetime(date_median).strftime('%Y-%m-%d')}")
                logger.info(f"      ‚Ä¢ Mode date: {pd.to_datetime(date_mode).strftime('%Y-%m-%d')}")
                logger.info(f"      ‚Ä¢ Mean date: {pd.to_datetime(date_mean).strftime('%Y-%m-%d')}")
                
                # Use mode for imputation (most frequent date)
                df['date'] = df['date'].fillna(date_mode)
                logger.info(f"   ‚úÖ Filled {unparseable_dates:,} missing dates with mode date: {pd.to_datetime(date_mode).strftime('%Y-%m-%d')}")
                
                # Add imputation flag
                df['date_imputed'] = df['date_original'].isna().astype(int)
            else:
                # Fallback to current date if no valid dates
                fallback_date = pd.Timestamp('2023-01-01')
                df['date'] = df['date'].fillna(fallback_date)
                df['date_imputed'] = 1
                logger.info(f"   ‚ö†Ô∏è  Used fallback date {fallback_date.date()} for all missing dates")
        else:
            df['date_imputed'] = 0
            logger.info(f"   ‚úÖ All dates successfully parsed - no imputation needed")
        
        # Date validation and quality checks
        logger.info(f"\nüéØ Step 4: Date Quality Validation")
        current_date = pd.Timestamp.now()
        earliest_reasonable_date = pd.Timestamp('2000-01-01')
        
        # Quality checks
        future_dates = (df['date'] > current_date).sum()
        very_old_dates = (df['date'] < earliest_reasonable_date).sum()
        reasonable_dates = len(df) - future_dates - very_old_dates
        
        logger.info(f"   üìä Date quality analysis:")
        logger.info(f"      ‚Ä¢ Reasonable dates: {reasonable_dates:,} ({(reasonable_dates/len(df)*100):.1f}%)")
        logger.info(f"      ‚Ä¢ Future dates: {future_dates:,} ({(future_dates/len(df)*100):.2f}%)")
        logger.info(f"      ‚Ä¢ Very old dates (pre-2000): {very_old_dates:,} ({(very_old_dates/len(df)*100):.2f}%)")
        
        # Add quality flags
        df['date_is_future'] = (df['date'] > current_date).astype(int)
        df['date_is_very_old'] = (df['date'] < earliest_reasonable_date).astype(int)
        
        # Final date statistics
        logger.info(f"\nüìä Final Date Processing Results:")
        logger.info(f"   ‚úÖ Date standardization complete")
        logger.info(f"   üìÖ Date range: {df['date'].min().date()} to {df['date'].max().date()}")
        logger.info(f"   üìà Date span: {(df['date'].max() - df['date'].min()).days:,} days")
        logger.info(f"   üéØ Processing success rate: {((len(df) - future_dates - very_old_dates)/len(df)*100):.1f}%")
        
        # Add temporal features
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['day_of_week'] = df['date'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        
        logger.info("\n‚úÖ Advanced date processing completed!")
        
        return df

    def advanced_amount_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Advanced amount processing with comprehensive currency normalization and intelligent imputation.
        
        Args:
            df: DataFrame with amount columns
            
        Returns:
            DataFrame with properly processed amounts
        """
        logger.info("üí∞ Advanced Amount Processing & Currency Normalization...")
        logger.info("=" * 50)
        
        if 'amount' not in df.columns:
            logger.warning("‚ö†Ô∏è  No 'amount' column found in DataFrame")
            return df
        
        # Store original amount for comparison
        df["amount_original"] = df["amount"].copy()
        
        logger.info("\nüéØ Step 1: Currency Format Analysis")
        # Analyze currency formats in the data
        amount_samples = df["amount"].dropna().astype(str).head(20).tolist()
        logger.info(f"   üìã Sample amount formats found:")
        unique_formats = set()
        for i, amount_sample in enumerate(amount_samples[:10], 1):
            logger.info(f"      {i:2d}. {amount_sample}")
            # Extract format pattern
            if "‚Çπ" in str(amount_sample):
                unique_formats.add("Rupee symbol (‚Çπ)")
            elif "$" in str(amount_sample):
                unique_formats.add("Dollar symbol ($)")
            elif "Rs" in str(amount_sample):
                unique_formats.add("Rs prefix")
            elif "," in str(amount_sample):
                unique_formats.add("Comma separated")
        
        logger.info(f"   üîç Detected currency formats: {list(unique_formats)}")
        
        logger.info("\nüéØ Step 2: Advanced Currency Cleaning")
        # Comprehensive currency symbol removal
        currency_patterns = [
            r"[‚Çπ$¬£‚Ç¨¬•Rs.]",      # Currency symbols
            r"INR|USD|EUR|GBP",  # Currency codes
            r"[,]",              # Thousands separators
            r"\s+"               # Extra whitespace
        ]
        
        df["amount_cleaned"] = df["amount"].astype(str)
        for pattern in currency_patterns:
            df["amount_cleaned"] = df["amount_cleaned"].str.replace(pattern, "", regex=True)
        
        df["amount_cleaned"] = df["amount_cleaned"].str.strip()
        
        # Convert to numeric
        df["amount"] = pd.to_numeric(df["amount_cleaned"], errors="coerce")
        
        # Analyze conversion results
        conversion_success = df["amount"].notna().sum()
        conversion_failures = df["amount"].isna().sum()
        conversion_rate = (conversion_success / len(df)) * 100
        
        logger.info(f"   ‚úÖ Currency conversion results:")
        logger.info(f"      ‚Ä¢ Successful conversions: {conversion_success:,} ({conversion_rate:.1f}%)")
        logger.info(f"      ‚Ä¢ Failed conversions: {conversion_failures:,} ({(conversion_failures/len(df)*100):.1f}%)")
        
        # Handle missing amounts with intelligent imputation
        if conversion_failures > 0:
            logger.info(f"\nüéØ Step 3: Intelligent Amount Imputation")
            
            valid_amounts = df["amount"].dropna()
            if len(valid_amounts) > 0:
                # Statistical measures for imputation
                amount_median = valid_amounts.median()
                amount_mean = valid_amounts.mean()
                amount_mode = valid_amounts.mode()[0] if not valid_amounts.mode().empty else amount_median
                
                logger.info(f"   üìä Amount statistics for imputation:")
                logger.info(f"      ‚Ä¢ Median: ‚Çπ{amount_median:,.2f}")
                logger.info(f"      ‚Ä¢ Mean: ‚Çπ{amount_mean:,.2f}")
                logger.info(f"      ‚Ä¢ Mode: ‚Çπ{amount_mode:,.2f}")
                
                # Use median for imputation (robust to outliers)
                df["amount"] = df["amount"].fillna(amount_median)
                logger.info(f"   ‚úÖ Filled {conversion_failures:,} missing amounts with median: ‚Çπ{amount_median:,.2f}")
                
                # Add imputation flag
                df["amount_imputed"] = df["amount_original"].isna().astype(int)
            else:
                df["amount"] = df["amount"].fillna(1000)  # Fallback value
                df["amount_imputed"] = 1
        else:
            df["amount_imputed"] = 0
        
        logger.info("\nüéØ Step 4: Amount Quality Validation")
        # Quality checks and corrections
        negative_amounts = (df["amount"] < 0).sum()
        zero_amounts = (df["amount"] == 0).sum()
        positive_amounts = (df["amount"] > 0).sum()
        
        logger.info(f"   üìä Amount quality analysis:")
        logger.info(f"      ‚Ä¢ Positive amounts: {positive_amounts:,} ({(positive_amounts/len(df)*100):.1f}%)")
        logger.info(f"      ‚Ä¢ Zero amounts: {zero_amounts:,} ({(zero_amounts/len(df)*100):.2f}%)")
        logger.info(f"      ‚Ä¢ Negative amounts: {negative_amounts:,} ({(negative_amounts/len(df)*100):.2f}%)")
        
        # Handle negative and zero amounts
        if negative_amounts > 0 or zero_amounts > 0:
            invalid_amounts = negative_amounts + zero_amounts
            median_replacement = df[df["amount"] > 0]["amount"].median() if positive_amounts > 0 else 1000
            
            df.loc[df["amount"] <= 0, "amount"] = median_replacement
            logger.info(f"   ‚úÖ Replaced {invalid_amounts:,} invalid amounts with median: ‚Çπ{median_replacement:,.2f}")
            
            # Add quality flags
            df["amount_was_invalid"] = ((df["amount_original"].astype(str).str.contains(r"^[0-]|^-", na=False)) | 
                                       (pd.to_numeric(df["amount_original"], errors="coerce") <= 0)).astype(int)
        else:
            df["amount_was_invalid"] = 0
        
        # Final amount statistics
        if len(df) > 0:
            logger.info(f"\nüìä Amount Processing Summary:")
            logger.info(f"   ‚Ä¢ Successfully processed: {conversion_success:,} amounts")
            logger.info(f"   ‚Ä¢ Amount range: ‚Çπ{df['amount'].min():.2f} to ‚Çπ{df['amount'].max():,.2f}")
            logger.info(f"   ‚Ä¢ Average amount: ‚Çπ{df['amount'].mean():,.2f}")
            logger.info(f"   ‚Ä¢ Median amount: ‚Çπ{df['amount'].median():,.2f}")
            logger.info(f"   ‚Ä¢ Total transaction value: ‚Çπ{df['amount'].sum():,.2f}")
        
        # Clean up temporary columns
        df.drop(['amount_cleaned'], axis=1, inplace=True, errors='ignore')
        
        logger.info("\n‚úÖ Advanced amount processing completed!")
        
        return df

    def _fuzzy_standardize(self, series: pd.Series, valid_list: List[str], threshold: int = 80) -> List[str]:
        """
        Apply fuzzy matching to standardize category names.
        
        Args:
            series: Series of category names to standardize
            valid_list: List of valid category names
            threshold: Minimum similarity score for matching
            
        Returns:
            List of standardized category names
        """
        cleaned = []
        for val in series:
            if pd.isna(val):
                cleaned.append("Others")
                continue
            
            if FUZZY_MATCHING_AVAILABLE:
                match, score, _ = process.extractOne(str(val), valid_list)
                cleaned.append(match if score >= threshold else "Others")
            else:
                # Fallback to exact matching
                val_lower = str(val).lower()
                best_match = None
                for valid_cat in valid_list:
                    if val_lower in valid_cat.lower() or valid_cat.lower() in val_lower:
                        best_match = valid_cat
                        break
                cleaned.append(best_match if best_match else "Others")
        
        return cleaned
    
    def standardize_categories(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Advanced category standardization using fuzzy matching and intelligent grouping.
        
        Args:
            df: DataFrame with category columns
            
        Returns:
            DataFrame with standardized categories
        """
        logger.info("üè∑Ô∏è  Advanced Category Standardization with Fuzzy Matching...")
        logger.info("=" * 50)
        
        if 'category' not in df.columns:
            logger.warning("‚ö†Ô∏è  No 'category' column found in DataFrame")
            return df
        
        # Keep original copy
        df['category_original'] = df['category'].astype(str).str.strip()
        
        # Analyze current categories
        logger.info("\nüéØ Step 1: Category Analysis")
        unique_categories = df['category'].value_counts()
        logger.info(f"   ‚Ä¢ Total unique categories: {len(unique_categories)}")
        logger.info(f"   ‚Ä¢ Top 10 categories:")
        for cat, count in unique_categories.head(10).items():
            percentage = (count / len(df)) * 100
            logger.info(f"     - {cat}: {count:,} transactions ({percentage:.1f}%)")
        
        # Define valid master categories (comprehensive list)
        valid_categories = [
            'Education', 'Housing', 'Income', 'Food & Dining', 'Entertainment',
            'Bills & Utilities', 'Others', 'Transportation', 'Healthcare', 
            'Travel', 'Savings', 'Shopping', 'Personal Care', 'Financial'
        ]
        
        logger.info(f"\nüéØ Step 2: Fuzzy Category Standardization")
        logger.info(f"   üéØ Using {'fuzzy matching' if FUZZY_MATCHING_AVAILABLE else 'basic string matching'}")
        
        # Apply fuzzy cleaning with intelligent matching
        df['category'] = self._fuzzy_standardize(df['category_original'], valid_categories, threshold=80)
        
        # Report standardization results
        cats_before = df['category_original'].nunique()
        cats_after = df['category'].nunique()
        df['category_was_standardized'] = (df['category_original'].str.lower() != df['category'].str.lower()).astype(int)
        
        standardized_count = df['category_was_standardized'].sum()
        reduction_percentage = ((cats_before - cats_after) / cats_before * 100) if cats_before > 0 else 0
        
        logger.info(f"\nüìä Category Standardization Results:")
        logger.info(f"   ‚úÖ Category standardization complete")
        logger.info(f"   üìä Categories reduced from {cats_before} to {cats_after} ({reduction_percentage:.1f}% reduction)")
        logger.info(f"   üîÑ Entries standardized: {standardized_count:,} ({(standardized_count/len(df)*100):.1f}%)")
        logger.info(f"   üìã Final categories: {sorted(df['category'].unique())}")
        
        # Final category distribution
        final_categories = df['category'].value_counts()
        logger.info(f"\n   ‚Ä¢ Standardized category distribution:")
        for cat, count in final_categories.items():
            percentage = (count / len(df)) * 100
            logger.info(f"     - {cat}: {count:,} transactions ({percentage:.1f}%)")
        
        return df
    
    def standardize_locations(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize location names using fuzzy matching.
        
        Args:
            df: DataFrame with location columns
            
        Returns:
            DataFrame with standardized locations
        """
        if 'location' not in df.columns:
            return df
        
        logger.info("üó∫Ô∏è  Standardizing Location Names...")
        
        # Keep original copy
        df['location_original'] = df['location'].astype(str).str.strip()
        
        # Define major Indian cities and common locations
        valid_locations = [
            'Mumbai', 'Delhi', 'Bangalore', 'Hyderabad', 'Ahmedabad', 'Chennai', 
            'Kolkata', 'Surat', 'Pune', 'Jaipur', 'Lucknow', 'Kanpur', 'Nagpur',
            'Indore', 'Thane', 'Bhopal', 'Visakhapatnam', 'Pimpri-Chinchwad',
            'Patna', 'Vadodara', 'Ghaziabad', 'Ludhiana', 'Agra', 'Nashik',
            'Online', 'Unknown', 'Others'
        ]
        
        # Apply fuzzy standardization
        df['location'] = self._fuzzy_standardize(df['location_original'], valid_locations, threshold=85)
        
        locations_before = df['location_original'].nunique()
        locations_after = df['location'].nunique()
        standardized_locations = (df['location_original'].str.lower() != df['location'].str.lower()).sum()
        
        logger.info(f"   ‚úÖ Locations reduced from {locations_before} to {locations_after}")
        logger.info(f"   üîÑ Location entries standardized: {standardized_locations:,}")
        
        return df
    
    def standardize_payment_modes(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize payment mode names using fuzzy matching.
        
        Args:
            df: DataFrame with payment_mode columns
            
        Returns:
            DataFrame with standardized payment modes
        """
        if 'payment_mode' not in df.columns:
            return df
        
        logger.info("üí≥ Standardizing Payment Modes...")
        
        # Keep original copy
        df['payment_mode_original'] = df['payment_mode'].astype(str).str.strip()
        
        # Define standard payment modes
        valid_payment_modes = [
            'Credit Card', 'Debit Card', 'Cash', 'Bank Transfer', 'UPI', 
            'Net Banking', 'Mobile Wallet', 'Cheque', 'Unknown', 'Others'
        ]
        
        # Apply fuzzy standardization
        df['payment_mode'] = self._fuzzy_standardize(df['payment_mode_original'], valid_payment_modes, threshold=75)
        
        modes_before = df['payment_mode_original'].nunique()
        modes_after = df['payment_mode'].nunique()
        standardized_modes = (df['payment_mode_original'].str.lower() != df['payment_mode'].str.lower()).sum()
        
        logger.info(f"   ‚úÖ Payment modes reduced from {modes_before} to {modes_after}")
        logger.info(f"   üîÑ Payment mode entries standardized: {standardized_modes:,}")
        
        return df

    def detect_and_handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Detect and handle outliers in amount data using statistical methods.
        
        Args:
            df: DataFrame with amount data
            
        Returns:
            DataFrame with outliers handled
        """
        logger.info("üìà Advanced Outlier Detection & Handling...")
        logger.info("=" * 50)
        
        if 'amount' not in df.columns:
            logger.warning("‚ö†Ô∏è  No 'amount' column found for outlier detection")
            return df
        
        outlier_method = self.config['preprocessing']['outlier_method']
        outlier_factor = self.config['preprocessing']['outlier_factor']
        
        logger.info(f"\nüéØ Step 1: Outlier Detection using {outlier_method.upper()} method")
        
        initial_count = len(df)
        
        lower_bound = None
        upper_bound = None
        
        if outlier_method == 'iqr':
            # IQR Method
            Q1 = df['amount'].quantile(0.25)
            Q3 = df['amount'].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - outlier_factor * IQR
            upper_bound = Q3 + outlier_factor * IQR
            
            logger.info(f"   ‚Ä¢ Q1: ‚Çπ{Q1:,.2f}")
            logger.info(f"   ‚Ä¢ Q3: ‚Çπ{Q3:,.2f}")
            logger.info(f"   ‚Ä¢ IQR: ‚Çπ{IQR:,.2f}")
            logger.info(f"   ‚Ä¢ Lower bound: ‚Çπ{lower_bound:,.2f}")
            logger.info(f"   ‚Ä¢ Upper bound: ‚Çπ{upper_bound:,.2f}")
            
            # Identify outliers
            outliers_mask = (df['amount'] < lower_bound) | (df['amount'] > upper_bound)
            
        elif outlier_method == 'zscore':
            # Z-Score Method
            try:
                z_scores = np.abs(stats.zscore(df['amount'].values))
                threshold = outlier_factor  # typically 2 or 3
                
                logger.info(f"   ‚Ä¢ Z-score threshold: {threshold}")
                logger.info(f"   ‚Ä¢ Mean amount: ‚Çπ{df['amount'].mean():,.2f}")
                logger.info(f"   ‚Ä¢ Std deviation: ‚Çπ{df['amount'].std():,.2f}")
                
                # Identify outliers
                outliers_mask = z_scores > threshold
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è  Z-score calculation failed: {e}. Falling back to IQR method.")
                # Fallback to IQR
                Q1 = df['amount'].quantile(0.25)
                Q3 = df['amount'].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - outlier_factor * IQR
                upper_bound = Q3 + outlier_factor * IQR
                outliers_mask = (df['amount'] < lower_bound) | (df['amount'] > upper_bound)
            
        else:
            logger.warning(f"   ‚ö†Ô∏è  Unknown outlier method: {outlier_method}. Using IQR method.")
            # Fallback to IQR
            Q1 = df['amount'].quantile(0.25)
            Q3 = df['amount'].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - outlier_factor * IQR
            upper_bound = Q3 + outlier_factor * IQR
            outliers_mask = (df['amount'] < lower_bound) | (df['amount'] > upper_bound)
        
        outlier_count = outliers_mask.sum()
        outlier_percentage = (outlier_count / len(df)) * 100
        
        logger.info(f"\nüéØ Step 2: Outlier Analysis")
        logger.info(f"   ‚Ä¢ Total outliers detected: {outlier_count:,} ({outlier_percentage:.2f}%)")
        
        if outlier_count > 0:
            # Analyze outliers
            outlier_amounts = df[outliers_mask]['amount']
            logger.info(f"   ‚Ä¢ Outlier amount range: ‚Çπ{outlier_amounts.min():,.2f} to ‚Çπ{outlier_amounts.max():,.2f}")
            logger.info(f"   ‚Ä¢ Average outlier amount: ‚Çπ{outlier_amounts.mean():,.2f}")
            
            # Flag outliers instead of removing them
            df['is_outlier'] = outliers_mask.astype(int)
            
            # Option to cap outliers instead of removing
            if outlier_percentage > 10:  # If too many outliers, cap them
                logger.info(f"   ‚Ä¢ Capping outliers (>10% of data)")
                if upper_bound is not None and lower_bound is not None:
                    df.loc[df['amount'] > upper_bound, 'amount'] = upper_bound
                    df.loc[df['amount'] < lower_bound, 'amount'] = lower_bound
                    logger.info(f"   ‚Ä¢ Outliers capped to bounds: ‚Çπ{lower_bound:,.2f} - ‚Çπ{upper_bound:,.2f}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è  Cannot cap outliers - bounds not defined")
            else:
                # Remove extreme outliers
                logger.info(f"   ‚Ä¢ Removing extreme outliers")
                df = df[~outliers_mask]
                
                removed_count = initial_count - len(df)
                logger.info(f"   ‚Ä¢ Removed {removed_count:,} outlier records")
        else:
            logger.info("   ‚úÖ No outliers detected")
            df['is_outlier'] = 0
        
        # Final statistics
        logger.info(f"\nüìä Outlier Handling Summary:")
        logger.info(f"   ‚Ä¢ Initial records: {initial_count:,}")
        logger.info(f"   ‚Ä¢ Final records: {len(df):,}")
        logger.info(f"   ‚Ä¢ Data retention: {(len(df)/initial_count)*100:.2f}%")
        
        if len(df) > 0:
            logger.info(f"   ‚Ä¢ Final amount range: ‚Çπ{df['amount'].min():,.2f} to ‚Çπ{df['amount'].max():,.2f}")
            logger.info(f"   ‚Ä¢ Final average amount: ‚Çπ{df['amount'].mean():,.2f}")
        
        return df

    def comprehensive_data_cleaning(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Execute comprehensive data cleaning pipeline.
        
        Args:
            df: Raw combined DataFrame
            
        Returns:
            Thoroughly cleaned DataFrame
        """
        logger.info("üßπ Comprehensive Data Cleaning Pipeline...")
        logger.info("=" * 60)
        
        # Store initial statistics
        initial_shape = df.shape
        logger.info(f"üöÄ Starting with {initial_shape[0]:,} records and {initial_shape[1]} columns")
        
        # Step 1: Advanced duplicate detection
        df = self.advanced_duplicate_detection(df)
        
        # Step 2: Advanced date processing
        df = self.advanced_date_processing(df)
        
        # Step 3: Advanced amount processing
        df = self.advanced_amount_processing(df)
        
        # Step 4: Category standardization with fuzzy matching
        df = self.standardize_categories(df)
        
        # Step 5: Location standardization
        df = self.standardize_locations(df)
        
        # Step 6: Payment mode standardization
        df = self.standardize_payment_modes(df)
        
        # Step 7: Outlier detection and handling
        df = self.detect_and_handle_outliers(df)
        
        # Step 6: Final data validation
        logger.info("‚úÖ Final Data Validation & Quality Check...")
        
        # Remove any remaining rows with critical missing values
        critical_columns = ['date', 'amount', 'category']
        before_critical_cleaning = len(df)
        df = df.dropna(subset=critical_columns)
        critical_cleaned = before_critical_cleaning - len(df)
        
        if critical_cleaned > 0:
            logger.info(f"   ‚Ä¢ Removed {critical_cleaned:,} records with missing critical data")
        
        # Final statistics
        final_shape = df.shape
        data_retention = (final_shape[0] / initial_shape[0]) * 100
        
        logger.info(f"\nüèÅ Comprehensive Cleaning Complete!")
        logger.info(f"=" * 60)
        logger.info(f"üìä Final Dataset Statistics:")
        logger.info(f"   ‚Ä¢ Initial records: {initial_shape[0]:,}")
        logger.info(f"   ‚Ä¢ Final records: {final_shape[0]:,}")
        logger.info(f"   ‚Ä¢ Data retention rate: {data_retention:.2f}%")
        logger.info(f"   ‚Ä¢ Columns: {final_shape[1]}")
        
        if len(df) > 0:
            logger.info(f"   ‚Ä¢ Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}")
            logger.info(f"   ‚Ä¢ Amount range: ‚Çπ{df['amount'].min():,.2f} to ‚Çπ{df['amount'].max():,.2f}")
            logger.info(f"   ‚Ä¢ Categories: {df['category'].nunique()} unique")
            logger.info(f"   ‚Ä¢ Total transaction value: ‚Çπ{df['amount'].sum():,.2f}")
            
            if 'user_id' in df.columns:
                logger.info(f"   ‚Ä¢ Unique users: {df['user_id'].nunique():,}")
        
        # Store processed data
        self.processed_data = df
        
        return df

    def prepare_time_series_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Prepare cleaned data for time series forecasting.
        
        Args:
            df: Cleaned transaction data
            
        Returns:
            Time series ready data
        """
        logger.info("üìà Preparing Time Series Data for Forecasting...")
        logger.info("=" * 50)
        
        # Create daily aggregations
        logger.info("üéØ Creating daily expense aggregations...")
        
        # Group by date and category for daily sums
        daily_data = df.groupby(['date', 'category']).agg({
            'amount': 'sum',
            'transaction_id': 'count'
        }).reset_index()
        
        daily_data.columns = ['date', 'category', 'daily_expense', 'transaction_count']
        
        # Create pivot table for category-wise time series
        pivot_data = daily_data.pivot(index='date', columns='category', values='daily_expense')
        pivot_data = pivot_data.fillna(0)
        
        # Add total daily expense
        pivot_data['total_daily_expense'] = pivot_data.sum(axis=1)
        
        # Reset index to make date a column
        pivot_data = pivot_data.reset_index()
        
        # Sort by date
        pivot_data = pivot_data.sort_values('date')
        
        logger.info(f"‚úÖ Time series data prepared:")
        logger.info(f"   ‚Ä¢ Date range: {pivot_data['date'].min().strftime('%Y-%m-%d')} to {pivot_data['date'].max().strftime('%Y-%m-%d')}")
        logger.info(f"   ‚Ä¢ Total days: {len(pivot_data):,}")
        logger.info(f"   ‚Ä¢ Categories: {len([col for col in pivot_data.columns if col not in ['date', 'total_daily_expense']])} + total")
        
        return pivot_data

    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Split time series data into train, validation, and test sets.
        
        Args:
            df: Time series data
            
        Returns:
            Tuple of (train, validation, test) DataFrames
        """
        logger.info("üîÑ Splitting data for training/validation/testing...")
        
        # Sort by date to ensure temporal order
        df_sorted = df.sort_values('date')
        
        # Calculate split points
        total_len = len(df_sorted)
        train_split = self.config['data']['train_split']
        val_split = self.config['data']['val_split']
        
        train_idx = int(total_len * train_split)
        val_idx = int(total_len * (train_split + val_split))
        
        # Split data
        train_data = df_sorted[:train_idx].copy()
        val_data = df_sorted[train_idx:val_idx].copy()
        test_data = df_sorted[val_idx:].copy()
        
        logger.info(f"‚úÖ Data split completed:")
        logger.info(f"   ‚Ä¢ Training: {len(train_data):,} days ({len(train_data)/total_len*100:.1f}%)")
        logger.info(f"   ‚Ä¢ Validation: {len(val_data):,} days ({len(val_data)/total_len*100:.1f}%)")
        logger.info(f"   ‚Ä¢ Testing: {len(test_data):,} days ({len(test_data)/total_len*100:.1f}%)")
        
        return train_data, val_data, test_data

    def save_processed_data(self, train_data: pd.DataFrame, val_data: pd.DataFrame, 
                          test_data: pd.DataFrame, processed_raw: Optional[pd.DataFrame] = None) -> None:
        """
        Save all processed data to files.
        
        Args:
            train_data: Training dataset
            val_data: Validation dataset  
            test_data: Test dataset
            processed_raw: Optional cleaned raw data
        """
        logger.info("üíæ Saving processed data files...")
        
        # Create processed directory
        self.processed_data_path.mkdir(parents=True, exist_ok=True)
        
        # Save time series splits
        train_data.to_csv(self.processed_data_path / "train_data.csv", index=False)
        val_data.to_csv(self.processed_data_path / "val_data.csv", index=False)
        test_data.to_csv(self.processed_data_path / "test_data.csv", index=False)
        
        logger.info("‚úÖ Time series data saved:")
        logger.info(f"   ‚Ä¢ {self.processed_data_path / 'train_data.csv'}")
        logger.info(f"   ‚Ä¢ {self.processed_data_path / 'val_data.csv'}")
        logger.info(f"   ‚Ä¢ {self.processed_data_path / 'test_data.csv'}")
        
        # Save cleaned raw data if provided
        if processed_raw is not None:
            processed_raw.to_csv(self.processed_data_path / "cleaned_transactions.csv", index=False)
            logger.info(f"   ‚Ä¢ {self.processed_data_path / 'cleaned_transactions.csv'}")
        
        # Save data summary
        summary = {
            'processing_date': datetime.now().isoformat(),
            'datasets_processed': list(self.datasets.keys()) if self.datasets else [],
            'total_original_records': sum(df.shape[0] for df in self.datasets.values()) if self.datasets else 0,
            'final_clean_records': len(processed_raw) if processed_raw is not None else 0,
            'time_series_days': len(train_data) + len(val_data) + len(test_data),
            'train_days': len(train_data),
            'val_days': len(val_data),
            'test_days': len(test_data)
        }
        
        import json
        with open(self.processed_data_path / "processing_summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"   ‚Ä¢ {self.processed_data_path / 'processing_summary.json'}")
        logger.info("üíæ All processed data saved successfully!")

    def run_complete_preprocessing_pipeline(self) -> None:
        """
        Execute the complete advanced preprocessing pipeline.
        """
        logger.info("üöÄ Starting Complete Advanced Preprocessing Pipeline...")
        logger.info("=" * 70)
        
        try:
            # Step 1: Load multiple datasets
            self.load_multiple_datasets()
            
            # Step 2: Merge datasets intelligently
            combined_data = self.merge_datasets()
            
            # Step 3: Comprehensive data cleaning
            cleaned_data = self.comprehensive_data_cleaning(combined_data)
            
            # Step 4: Prepare time series data
            time_series_data = self.prepare_time_series_data(cleaned_data)
            
            # Step 5: Split data for modeling
            train_data, val_data, test_data = self.split_data(time_series_data)
            
            # Step 6: Save all processed data
            self.save_processed_data(train_data, val_data, test_data, cleaned_data)
            
            # Final success message
            logger.info("\nüéâ PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info("=" * 70)
            logger.info("‚úÖ All data has been processed, cleaned, and prepared for modeling")
            logger.info(f"üìÅ Processed data saved to: {self.processed_data_path}")
            logger.info("üöÄ Ready for feature engineering and model training!")
            
        except Exception as e:
            logger.error(f"‚ùå Preprocessing pipeline failed: {str(e)}")
            raise


def main():
    """Main function to run the advanced preprocessing pipeline."""
    try:
        # Initialize preprocessor
        preprocessor = AdvancedDataPreprocessor()
        
        # Run complete pipeline
        preprocessor.run_complete_preprocessing_pipeline()
        
    except Exception as e:
        logger.error(f"Failed to run preprocessing pipeline: {str(e)}")
        raise


if __name__ == "__main__":
    main()